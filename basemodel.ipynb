{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7583a09b",
   "metadata": {},
   "source": [
    "# Implementing Biomedical NER with TinyLlama \n",
    "\n",
    "This notebook implements the core workflows presented in the paper **\"LLMs in Biomedical: A Study on Named Entity Recognition\"**. I will adapt the paper's methods for an open-source model, replacing the proprietary GPT-4 with **TinyLlama**.\n",
    "\n",
    "The goal is to perform **Named Entity Recognition (NER)** on biomedical text, identifying entities like diseases, treatments, and tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf25b5",
   "metadata": {},
   "source": [
    "--- \n",
    "## 1. Setup and Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2685616f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:35:45.058515Z",
     "start_time": "2025-10-22T02:35:19.028853Z"
    }
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.externals.array_api_compat.dask.array import astype\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from transformers import pipeline\n",
    "import os\n",
    "import requests\n",
    "import  ast\n",
    "\n",
    "# Check for GPU availability for faster processing\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\apoor\\projects\\NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "d3464c07",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Loading Models and Data\n",
    "\n",
    "Here, we load the TinyLlama chat model, the BioClinicalBERT model for embeddings."
   ]
  },
  {
   "cell_type": "code",
   "id": "8a01ebfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:35:45.137619Z",
     "start_time": "2025-10-22T02:35:45.131468Z"
    }
   },
   "source": [
    "os.getenv(\".env\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "3a8cc019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:36:07.188928Z",
     "start_time": "2025-10-22T02:35:45.162063Z"
    }
   },
   "source": [
    "# loading the tiny llama model \n",
    "pipe = pipeline('text-generation', model=\"meta-llama/Llama-3.2-3B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# loading the BioClinicalBert model for encodings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "print(\"Models Loaded Successfully!!\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.02s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Loaded Successfully!!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:36:08.276712Z",
     "start_time": "2025-10-22T02:36:08.251855Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "b85e04e786cf0674",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "7c6feaef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:36:08.633880Z",
     "start_time": "2025-10-22T02:36:08.443008Z"
    }
   },
   "source": [
    "# Convert data to a dataframe\n",
    "db = pd.read_csv(\"data-words/train.tsv\", sep='\\t')\n",
    "db = db.dropna()\n",
    "db"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       Identification  O\n",
       "0                  of  O\n",
       "1                APC2  O\n",
       "2                   ,  O\n",
       "3                   a  O\n",
       "4           homologue  O\n",
       "...               ... ..\n",
       "135994            and  O\n",
       "135995      increased  O\n",
       "135996       survival  O\n",
       "135997              .  O\n",
       "135998              .  O\n",
       "\n",
       "[135971 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Identification</th>\n",
       "      <th>O</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>APC2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>homologue</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135994</th>\n",
       "      <td>and</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135995</th>\n",
       "      <td>increased</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135996</th>\n",
       "      <td>survival</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135997</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135998</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135971 rows × 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "5877278b",
   "metadata": {},
   "source": [
    "# TANL"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating a function which inputs the search term and the database. It then looks up for the term for documents and returns the list of the documents. These documents will be used by the LLm as a context for identifying entities.",
   "id": "ed71ef8a0a60aa0d"
  },
  {
   "cell_type": "code",
   "id": "2d086b1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:36:08.672358Z",
     "start_time": "2025-10-22T02:36:08.656893Z"
    }
   },
   "source": [
    "\n",
    "def get_context(search_term,search_db):\n",
    "\n",
    "    BASE_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/\"\n",
    "\n",
    "    esearch_url = BASE_URL + \"esearch.fcgi\"\n",
    "\n",
    "    esearch_params = {\n",
    "        \"db\": search_db,\n",
    "        \"term\": search_term,\n",
    "        \"retmax\": 5,\n",
    "        \"retmode\": \"json\",\n",
    "        \"usehistory\": \"y\",\n",
    "        \"tool\": \"MyPubMedAPIScript\",\n",
    "        \"email\": os.getenv(\"email\"),\n",
    "        \"api_key\": os.getenv(\"ncbi_token\")\n",
    "    }\n",
    "\n",
    "    response = requests.get(esearch_url,params=esearch_params)\n",
    "    response.raise_for_status\n",
    "\n",
    "    result_json = response.json()\n",
    "\n",
    "    result = result_json.get(\"esearchresult\",{})\n",
    "    ids = result.get(\"idlist\",[])\n",
    "    count = result.get(\"count\",0)\n",
    "    webenv = result.get(\"webenv\")\n",
    "\n",
    "    final_result = {\"search term\": search_term,\n",
    "                    \"total results:\": count,\n",
    "                    \"id list\": ids,\n",
    "                    \"web environment\": webenv}\n",
    "\n",
    "\n",
    "    return final_result\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Creating a test for testing the first chunk with 1500 words\n",
    "### Results:- Best performance with 25 batch size"
   ],
   "id": "dea1354a02718f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T03:36:28.924729Z",
     "start_time": "2025-10-22T03:33:33.197088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Writing the prompt usning TANL technique\n",
    "\n",
    "current_words = db[\"Identification\"].iloc[0:1500]\n",
    "current_text = \" \".join(current_words)\n",
    "\n",
    "prompt_tanl = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an expert in medical domain. Given the following document, your task is to identify that it could potentially be a medical entity, do not add any other text just return the output format specified. The output should be a list of strings where the strings will be the potential medical entities nothing else, for example: the output format should be: ['entity 1', 'entity 2' ... and so on]\"\n",
    "     },\n",
    "    {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"data: {current_text}\"\n",
    "    },\n",
    "    ]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(prompt_tanl, tokenize=False, add_generation_prompt=True)\n",
    "test_entities_from_doc = pipe(prompt,\n",
    "        max_new_tokens=1000,\n",
    "        temperature=0.1,\n",
    "        batch_size=16,\n",
    "        return_full_text=False)"
   ],
   "id": "88ad89bb78c07717",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T03:37:25.833081Z",
     "start_time": "2025-10-22T03:37:25.827950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_result_preclassification = ast.literal_eval(test_entities_from_doc[0][\"generated_text\"])\n",
    "print(test_result_preclassification)"
   ],
   "id": "f06b6713356a85ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adenomatous polyposis coli', 'Wnt signalling pathway', 'glycogen synthase kinase 3beta', 'axin/conductin', 'betacatenin', 'Tcf-4 transcription factor', 'MSH2 mutation', 'hereditary non-polyposis cancer syndrome', 'HNPCC', 'colorectal cancer', 'Huntington disease', 'apolipoprotein E', 'CAG repeat length', 'complement component 7', 'cartilage-hair hypoplasia', 'dihydropyrimidine dehydrogenase deficiency']\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T03:38:14.050276Z",
     "start_time": "2025-10-22T03:38:01.115325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "list_of_context_file = []\n",
    "for i in test_result_preclassification:\n",
    "    result_from_umls = get_context(i,\"pubmed\")\n",
    "    list_of_context_file.append(result_from_umls)"
   ],
   "id": "85df6d0096d515cd",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T03:38:23.852747Z",
     "start_time": "2025-10-22T03:38:23.837389Z"
    }
   },
   "cell_type": "code",
   "source": "list_of_context_file",
   "id": "16ec9de8da2b8dbb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'search term': 'adenomatous polyposis coli',\n",
       "  'total results:': '10671',\n",
       "  'id list': ['41118559', '41112420', '41104667', '41067867', '41045501'],\n",
       "  'web environment': 'MCID_68f8519ac0fb9ce1550a7395'},\n",
       " {'search term': 'Wnt signalling pathway',\n",
       "  'total results:': '33267',\n",
       "  'id list': ['41110101', '41103766', '41103435', '41102854', '41102431'],\n",
       "  'web environment': 'MCID_68f8519b785fe9b67b06592a'},\n",
       " {'search term': 'glycogen synthase kinase 3beta',\n",
       "  'total results:': '10072',\n",
       "  'id list': ['41113442', '41105605', '41091591', '41086612', '41084480'],\n",
       "  'web environment': 'MCID_68f8519cca8d189b560fc01b'},\n",
       " {'search term': 'axin/conductin',\n",
       "  'total results:': '12',\n",
       "  'id list': ['21498506', '19966865', '18854359', '12183362', '12023307'],\n",
       "  'web environment': 'MCID_68f8519d0e7384fd3a0640f3'},\n",
       " {'search term': 'betacatenin',\n",
       "  'total results:': '33292',\n",
       "  'id list': ['41115563', '41113613', '41111081', '41110481', '41110101'],\n",
       "  'web environment': 'MCID_68f8519de46b8848fc032201'},\n",
       " {'search term': 'Tcf-4 transcription factor',\n",
       "  'total results:': '4869',\n",
       "  'id list': ['41114119', '41110296', '41106272', '41098715', '41094529'],\n",
       "  'web environment': 'MCID_68f8519eab0eccd1ec03f94b'},\n",
       " {'search term': 'MSH2 mutation',\n",
       "  'total results:': '4349',\n",
       "  'id list': ['41113919', '41096581', '41064077', '41061386', '41058101'],\n",
       "  'web environment': 'MCID_68f8519f599be59fac0484cb'},\n",
       " {'search term': 'hereditary non-polyposis cancer syndrome',\n",
       "  'total results:': '654',\n",
       "  'id list': ['40771437', '40744876', '40612862', '40580410', '40554495'],\n",
       "  'web environment': 'MCID_68f851a0caddc6ac70013caf'},\n",
       " {'search term': 'HNPCC',\n",
       "  'total results:': '8835',\n",
       "  'id list': ['41108504', '41096581', '41075302', '41054367', '41044115'],\n",
       "  'web environment': 'MCID_68f851a19c2d4a542a074ccd'},\n",
       " {'search term': 'colorectal cancer',\n",
       "  'total results:': '334323',\n",
       "  'id list': ['41118684', '41118374', '41118325', '41118193', '41118161'],\n",
       "  'web environment': 'MCID_68f851a1e344634a930107df'},\n",
       " {'search term': 'Huntington disease',\n",
       "  'total results:': '27595',\n",
       "  'id list': ['41116590', '41115590', '41115211', '41112198', '41111799'],\n",
       "  'web environment': 'MCID_68f851a2e54168d8d20cff18'},\n",
       " {'search term': 'apolipoprotein E',\n",
       "  'total results:': '32000',\n",
       "  'id list': ['41114585', '41114448', '41113397', '41113022', '41111608'],\n",
       "  'web environment': 'MCID_68f851a3d91c69f9700019dd'},\n",
       " {'search term': 'CAG repeat length',\n",
       "  'total results:': '2135',\n",
       "  'id list': ['41097020', '41085742', '41077586', '41074680', '41071260'],\n",
       "  'web environment': 'MCID_68f851a417902276910a8afd'},\n",
       " {'search term': 'complement component 7',\n",
       "  'total results:': '1050',\n",
       "  'id list': ['41101219', '41031136', '40997984', '40943308', '40885518'],\n",
       "  'web environment': 'MCID_68f851a5dd331772c60b8d6e'},\n",
       " {'search term': 'cartilage-hair hypoplasia',\n",
       "  'total results:': '276',\n",
       "  'id list': ['40808259', '22420014', '40110983', '39886981', '39321258'],\n",
       "  'web environment': 'MCID_68f851a5a5a3f10ef8034eb3'},\n",
       " {'search term': 'dihydropyrimidine dehydrogenase deficiency',\n",
       "  'total results:': '544',\n",
       "  'id list': ['40960875', '40943060', '40939752', '40912527', '40887140'],\n",
       "  'web environment': 'MCID_68f851a67170f43060059ff2'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final LLM calling for the entire document",
   "id": "cd64c257915d61ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Creating chinks of the column of size 1500 and adding them to the list of prompts",
   "id": "8a3bb532cca6591"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T03:27:55.535084Z",
     "start_time": "2025-10-22T03:27:55.469568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Writing the prompt usning TANL technique\n",
    "\n",
    "chunks = 1500\n",
    "overlap = 200\n",
    "all_potential_entities = set()\n",
    "all_prompts = []\n",
    "\n",
    "for i in range(0,len(db[\"Identification\"]),chunks-overlap):\n",
    "\n",
    "        current_words = db[\"Identification\"][i:i+chunks]\n",
    "        current_text = \" \".join(current_words)\n",
    "\n",
    "        prompt_tanl = [\n",
    "            {\n",
    "              \"role\": \"system\",\n",
    "              \"content\": \"You are an expert in medical domain. Given the following document, your task is to identify that it could potentially be a medical entity, do not add any other text just return the output format specified. The output should be a list of strings where the strings will be the potential medical entities nothing else, for example: the output format should be: ['entity 1', 'entity 2' ... and so on]\"\n",
    "                        },\n",
    "                {\n",
    "                 \"role\": \"user\",\n",
    "                 \"content\": f\"data: {current_text}\"\n",
    "                },\n",
    "                ]\n",
    "\n",
    "        prompt = pipe.tokenizer.apply_chat_template(prompt_tanl, tokenize=False, add_generation_prompt=True)\n",
    "        all_prompts.append(prompt)\n",
    "\n",
    "print(\"All prompts added\")\n",
    "pipe.tokenizer.pad_token = pipe.tokenizer.eos_token\n"
   ],
   "id": "4224fdbcdd260c83",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All prompts added\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Calling the pipeline",
   "id": "62d76f164b3fe498"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T03:30:57.719950Z",
     "start_time": "2025-10-22T03:30:56.736203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "entities_from_doc = pipe(all_prompts,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.1,\n",
    "        return_full_text=False,\n",
    "        batch_size=8)"
   ],
   "id": "c31a3af1b27d7a98",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m entities_from_doc = \u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_prompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m        \u001B[49m\u001B[43mmax_new_tokens\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_full_text\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m8\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:332\u001B[39m, in \u001B[36mTextGenerationPipeline.__call__\u001B[39m\u001B[34m(self, text_inputs, **kwargs)\u001B[39m\n\u001B[32m    330\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    331\u001B[39m                 \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m(\u001B[38;5;28mlist\u001B[39m(chats), **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m332\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtext_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1448\u001B[39m, in \u001B[36mPipeline.__call__\u001B[39m\u001B[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[39m\n\u001B[32m   1444\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m can_use_iterator:\n\u001B[32m   1445\u001B[39m     final_iterator = \u001B[38;5;28mself\u001B[39m.get_iterator(\n\u001B[32m   1446\u001B[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001B[32m   1447\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1448\u001B[39m     outputs = \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfinal_iterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1449\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n\u001B[32m   1450\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    123\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_item()\n\u001B[32m    125\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m126\u001B[39m item = \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    127\u001B[39m processed = \u001B[38;5;28mself\u001B[39m.infer(item, **\u001B[38;5;28mself\u001B[39m.params)\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:127\u001B[39m, in \u001B[36mPipelineIterator.__next__\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    125\u001B[39m \u001B[38;5;66;03m# We're out of items within a batch\u001B[39;00m\n\u001B[32m    126\u001B[39m item = \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m.iterator)\n\u001B[32m--> \u001B[39m\u001B[32m127\u001B[39m processed = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43minfer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    128\u001B[39m \u001B[38;5;66;03m# We now have a batch of \"inferred things\".\u001B[39;00m\n\u001B[32m    129\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.loader_batch_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    130\u001B[39m     \u001B[38;5;66;03m# Try to infer the size of the batch\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1374\u001B[39m, in \u001B[36mPipeline.forward\u001B[39m\u001B[34m(self, model_inputs, **forward_params)\u001B[39m\n\u001B[32m   1372\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[32m   1373\u001B[39m         model_inputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_inputs, device=\u001B[38;5;28mself\u001B[39m.device)\n\u001B[32m-> \u001B[39m\u001B[32m1374\u001B[39m         model_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1375\u001B[39m         model_outputs = \u001B[38;5;28mself\u001B[39m._ensure_tensor_on_device(model_outputs, device=torch.device(\u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m))\n\u001B[32m   1376\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:432\u001B[39m, in \u001B[36mTextGenerationPipeline._forward\u001B[39m\u001B[34m(self, model_inputs, **generate_kwargs)\u001B[39m\n\u001B[32m    429\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m generate_kwargs:\n\u001B[32m    430\u001B[39m     generate_kwargs[\u001B[33m\"\u001B[39m\u001B[33mgeneration_config\u001B[39m\u001B[33m\"\u001B[39m] = \u001B[38;5;28mself\u001B[39m.generation_config\n\u001B[32m--> \u001B[39m\u001B[32m432\u001B[39m output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mgenerate_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    434\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, ModelOutput):\n\u001B[32m    435\u001B[39m     generated_sequence = output.sequences\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001B[39m, in \u001B[36mcontext_decorator.<locals>.decorate_context\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    113\u001B[39m \u001B[38;5;129m@functools\u001B[39m.wraps(func)\n\u001B[32m    114\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mdecorate_context\u001B[39m(*args, **kwargs):\n\u001B[32m    115\u001B[39m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[32m--> \u001B[39m\u001B[32m116\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2539\u001B[39m, in \u001B[36mGenerationMixin.generate\u001B[39m\u001B[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001B[39m\n\u001B[32m   2528\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m GenerationMixin.generate(\n\u001B[32m   2529\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   2530\u001B[39m         inputs,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2534\u001B[39m         **kwargs,\n\u001B[32m   2535\u001B[39m     )\n\u001B[32m   2537\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001B[32m   2538\u001B[39m     \u001B[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2539\u001B[39m     result = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sample\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2540\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2541\u001B[39m \u001B[43m        \u001B[49m\u001B[43mlogits_processor\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_logits_processor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2542\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstopping_criteria\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprepared_stopping_criteria\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2543\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgeneration_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2544\u001B[39m \u001B[43m        \u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m=\u001B[49m\u001B[43msynced_gpus\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2545\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstreamer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2546\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2547\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2549\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m generation_mode \u001B[38;5;129;01min\u001B[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001B[32m   2550\u001B[39m     \u001B[38;5;66;03m# 11. run beam sample\u001B[39;00m\n\u001B[32m   2551\u001B[39m     result = \u001B[38;5;28mself\u001B[39m._beam_search(\n\u001B[32m   2552\u001B[39m         input_ids,\n\u001B[32m   2553\u001B[39m         logits_processor=prepared_logits_processor,\n\u001B[32m   (...)\u001B[39m\u001B[32m   2557\u001B[39m         **model_kwargs,\n\u001B[32m   2558\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:2867\u001B[39m, in \u001B[36mGenerationMixin._sample\u001B[39m\u001B[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001B[39m\n\u001B[32m   2864\u001B[39m model_inputs.update({\u001B[33m\"\u001B[39m\u001B[33moutput_hidden_states\u001B[39m\u001B[33m\"\u001B[39m: output_hidden_states} \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states \u001B[38;5;28;01melse\u001B[39;00m {})\n\u001B[32m   2866\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m is_prefill:\n\u001B[32m-> \u001B[39m\u001B[32m2867\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[32m   2868\u001B[39m     is_prefill = \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[32m   2869\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:940\u001B[39m, in \u001B[36mcan_return_tuple.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    938\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    939\u001B[39m     return_dict = return_dict_passed\n\u001B[32m--> \u001B[39m\u001B[32m940\u001B[39m output = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    941\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    942\u001B[39m     output = output.to_tuple()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:459\u001B[39m, in \u001B[36mLlamaForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m    427\u001B[39m \u001B[38;5;129m@can_return_tuple\u001B[39m\n\u001B[32m    428\u001B[39m \u001B[38;5;129m@auto_docstring\u001B[39m\n\u001B[32m    429\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m   (...)\u001B[39m\u001B[32m    440\u001B[39m     **kwargs: Unpack[TransformersKwargs],\n\u001B[32m    441\u001B[39m ) -> CausalLMOutputWithPast:\n\u001B[32m    442\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    443\u001B[39m \u001B[33;03m    Example:\u001B[39;00m\n\u001B[32m    444\u001B[39m \n\u001B[32m   (...)\u001B[39m\u001B[32m    457\u001B[39m \u001B[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001B[39;00m\n\u001B[32m    458\u001B[39m \u001B[33;03m    ```\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m459\u001B[39m     outputs: BaseModelOutputWithPast = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    461\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    462\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    463\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    464\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    465\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    466\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    468\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    470\u001B[39m     hidden_states = outputs.last_hidden_state\n\u001B[32m    471\u001B[39m     \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001B[39m, in \u001B[36mcheck_model_inputs.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1061\u001B[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001B[32m   1062\u001B[39m                 monkey_patched_layers.append((module, original_forward))\n\u001B[32m-> \u001B[39m\u001B[32m1064\u001B[39m outputs = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1065\u001B[39m \u001B[38;5;66;03m# Restore original forward methods\u001B[39;00m\n\u001B[32m   1066\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module, original_forward \u001B[38;5;129;01min\u001B[39;00m monkey_patched_layers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:395\u001B[39m, in \u001B[36mLlamaModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001B[39m\n\u001B[32m    392\u001B[39m position_embeddings = \u001B[38;5;28mself\u001B[39m.rotary_emb(hidden_states, position_ids)\n\u001B[32m    394\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m decoder_layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.layers[: \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers]:\n\u001B[32m--> \u001B[39m\u001B[32m395\u001B[39m     hidden_states = \u001B[43mdecoder_layer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    396\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcausal_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    398\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    399\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    400\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    401\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    402\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    403\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    405\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.norm(hidden_states)\n\u001B[32m    406\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m BaseModelOutputWithPast(\n\u001B[32m    407\u001B[39m     last_hidden_state=hidden_states,\n\u001B[32m    408\u001B[39m     past_key_values=past_key_values,\n\u001B[32m    409\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning_once(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:294\u001B[39m, in \u001B[36mLlamaDecoderLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001B[39m\n\u001B[32m    292\u001B[39m hidden_states = \u001B[38;5;28mself\u001B[39m.input_layernorm(hidden_states)\n\u001B[32m    293\u001B[39m \u001B[38;5;66;03m# Self Attention\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m294\u001B[39m hidden_states, _ = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    295\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    296\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    297\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    298\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    299\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    300\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    301\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_embeddings\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    302\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    303\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    304\u001B[39m hidden_states = residual + hidden_states\n\u001B[32m    306\u001B[39m \u001B[38;5;66;03m# Fully Connected\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\projects\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:263\u001B[39m, in \u001B[36mLlamaAttention.forward\u001B[39m\u001B[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001B[39m\n\u001B[32m    250\u001B[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001B[38;5;28mself\u001B[39m.config._attn_implementation]\n\u001B[32m    252\u001B[39m attn_output, attn_weights = attention_interface(\n\u001B[32m    253\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    254\u001B[39m     query_states,\n\u001B[32m   (...)\u001B[39m\u001B[32m    260\u001B[39m     **kwargs,\n\u001B[32m    261\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m263\u001B[39m attn_output = \u001B[43mattn_output\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minput_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m.contiguous()\n\u001B[32m    264\u001B[39m attn_output = \u001B[38;5;28mself\u001B[39m.o_proj(attn_output)\n\u001B[32m    265\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m attn_output, attn_weights\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Converting the result generated by the pipeline into lists and adding those lists to a set to prevent duplicate values",
   "id": "638e630215931fd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T03:30:23.496111Z",
     "start_time": "2025-10-22T03:30:23.425289Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, result in  enumerate(entities_from_doc):\n",
    "    res = result[0]['generated_text']\n",
    "    res_list = ast.literal_eval(res)\n",
    "    if isinstance(res_list, list):\n",
    "        all_potential_entities.update(res_list)"
   ],
   "id": "c280479f7baa2fe2",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'entities_from_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[16]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i, result \u001B[38;5;129;01min\u001B[39;00m  \u001B[38;5;28menumerate\u001B[39m(\u001B[43mentities_from_doc\u001B[49m):\n\u001B[32m      2\u001B[39m     res = result[\u001B[32m0\u001B[39m][\u001B[33m'\u001B[39m\u001B[33mgenerated_text\u001B[39m\u001B[33m'\u001B[39m]\n\u001B[32m      3\u001B[39m     res_list = ast.literal_eval(res)\n",
      "\u001B[31mNameError\u001B[39m: name 'entities_from_doc' is not defined"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T03:27:12.004025Z",
     "start_time": "2025-10-22T03:27:11.993751Z"
    }
   },
   "cell_type": "code",
   "source": "all_potential_entities",
   "id": "c2bd42f94bef1010",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T02:37:45.986514800Z",
     "start_time": "2025-10-20T20:01:25.869943Z"
    }
   },
   "cell_type": "code",
   "source": "result_from_umls = get_context(\"Wilson disease\",\"pubmed\")",
   "id": "2a3838ed8d646f16",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "a98afd48",
   "metadata": {},
   "source": [
    "# Dice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
